# E-commerce-ETL-Pipeline

## Project Overview

This project is an **end-to-end ETL (Extract, Transform, Load) pipeline** that simulates a real-world e-commerce data workflow. It demonstrates how raw data from an external API can be **processed, cleaned, and stored** in a relational database for analytics or reporting purposes. The pipeline is designed to be **modular, reusable, and easy to extend** for other data sources or projects.

---

## Project Description

The ETL pipeline performs the following key tasks:

- ðŸ”¹ **Extract** product data from the [DummyJSON API](https://dummyjson.com/), fetching raw JSON data.

- ðŸ”¹ **Transform** the data by **cleaning missing values**, **normalizing fields**, and **standardizing formats** to match a relational schema.

- ðŸ”¹ **Load** the transformed data into a structured format, either as **CSV files** or directly into a **PostgreSQL database** for analytics-ready use.

This project highlights practical **data engineering skills**, including:

- âœ… **API data extraction** and handling of JSON data  
- âœ… **Data cleaning, normalization, and transformation** with Python & Pandas  
- âœ… **Loading and mapping data** into relational databases using SQL and `psycopg2`  

It is part of my **Data Engineering learning journey**, showcasing hands-on experience in building a pipeline that bridges raw data sources and structured storage for analysis.

---

## Technologies Used  
- **Python** â†’ scripting & data processing  
- **Pandas** â†’ data cleaning and transformation  
- **psycopg2** â†’ PostgreSQL connection from Python  
- **PostgreSQL** â†’ relational database for structured storage  
- **Git & GitHub** â†’ version control and project hosting  
- **pgAdmin4** â†’ database management and ERD visualization  

---

## Project Structure  
```
E-commerce-ETL-Pipeline/
â”‚
â”œâ”€â”€ data/ # raw & transformed data (CSV files)
â”‚ â””â”€â”€ transformed_products.csv
â”‚
â”œâ”€â”€ src/ # source code for ETL
â”‚ â”œâ”€â”€ Extract.py # extracts data from API
â”‚ â”œâ”€â”€ Transform.py # cleans/transforms data
â”‚ â”œâ”€â”€ Load.py # loads data into PostgreSQL
â”‚
â”‚â”€â”€ images/
â”‚ â””â”€â”€ schema.png # ERD Diagram
â”‚
â”œâ”€â”€ sql/
â”‚ â””â”€â”€ schema.sql # SQL script for creating tables
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # project documentation
```
## Database Schema & Design  

Below is the schema design for this project:  

![Database Schema](./images/schema.png)  

---
### Tables  

- **`products`** â†’ Stores core product information such as product name, price, discount percentage, stock, and SKU (unique identifier).  

- **`product_details`** â†’ Holds extended product attributes (e.g., dimensions, weight, warranty, shipping details, availability, return policy, minimum order quantity, notes).  

- **`categories`** â†’ Defines broad product categories (e.g., *Electronics, Fashion, Home Appliances*).  

- **`subcategories`** â†’ Represents more specific categories under a main category (e.g., *Mobiles, Laptops* under *Electronics*).  

- **`product_category_mapping`** â†’ Acts as a **junction table** linking `products` to both `categories`, `subcategories` and `product_details` for flexible many-to-many relationships.  

---
## ETL Workflow  

The pipeline follows the **Extract â†’ Transform â†’ Load (ETL)** process:  

- **Extract**  
  - Fetches product data from the [DummyJSON API](https://dummyjson.com/).  
  - Data is ingested in JSON format.  

- **Transform**  
  - Cleans and normalizes fields.  
  - Handles missing values.  
  - Renames columns for consistency with database schema.  
  - Stores the transformed dataset as a CSV file (`transformed_products.csv`).  

- **Load**  
  - Creates database schema from `sql/schema.sql`.  
  - Loads the cleaned data into **PostgreSQL** using `psycopg2`.  
  - Establishes relational mappings between products, product_details, categories, and subcategories.
 
---
## Setup & Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/your-username/E-commerce-ETL-Pipeline.git
cd E-commerce-ETL-Pipeline
pip install -r requirements.txt
```
---
## Usage Example

Run the ETL pipeline in order:

```bash
python src/Extract.py
python src/Transform.py
python src/Load.py
```

---
## Contributing
Contributions are welcome! Please open an issue or submit a pull request.

## Author

**Amankwe Amarachi Francisca**  
Data Engineering Enthusiast | Python & SQL | Aspiring Data Engineer  
[LinkedIn](https://www.linkedin.com/in/amankwe-amarachi/) | [GitHub](https://github.com/Anabelmara18)

